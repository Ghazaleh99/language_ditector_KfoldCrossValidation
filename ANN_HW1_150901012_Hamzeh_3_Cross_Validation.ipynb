{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "FOYS74oy49x7",
        "yU8qXJcL5JH3",
        "g_4Ugzyl5M2-",
        "TLHr56FjBk6J",
        "LntWuwrVFCQp",
        "NNj6BNfwP_84",
        "YgmWnvPaRn-B"
      ],
      "authorship_tag": "ABX9TyPRKStXHKvU1LGjwm/aF7q6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ghazaleh99/language_ditector_KfoldCrossValidation/blob/main/ANN_HW1_150901012_Hamzeh_3_Cross_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#تمرین یک درس شبکه های عصبی\n",
        "#5-fold Cross-Validation\n",
        "###**سوال سوم**\n",
        "\n",
        "غزاله حمزه\n",
        "\n",
        "##قسمت ج\n"
      ],
      "metadata": {
        "id": "jv21Md-IZ7gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1 - Import the library"
      ],
      "metadata": {
        "id": "nxbF6IAPTwso"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggdih9wbNofK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2 - Setting up the Data"
      ],
      "metadata": {
        "id": "hWIfm6G4T6ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "langD_file_path = 'ANN-HW1-Data-LangID.xlsx'\n",
        "lang_data = pd.read_excel(langD_file_path)\n",
        "lang_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-ifTG7rTsNj",
        "outputId": "3325f1a2-3cfb-4f19-dcae-53d079fa5955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['جملات', 'زبان'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3 - Splitting the Data"
      ],
      "metadata": {
        "id": "xvNTEx3VUTWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold = 5\n",
        "data_shape = lang_data.shape\n",
        "print('Data shape is',data_shape)\n",
        "validation_size = int(data_shape[0]) // k_fold \n",
        "print('valdation size', validation_size)"
      ],
      "metadata": {
        "id": "AV_oJid2r3lw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5af4617-c852-41c8-8b1f-a65f057920d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape is (150, 2)\n",
            "valdation size 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####taking one sentence from evrey language to make a good k_fold dataset!"
      ],
      "metadata": {
        "id": "JRZ_vpw6POc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = (k_fold-1)*validation_size\n",
        "test_size = validation_size\n",
        "data_size = int(data_shape[0])\n",
        "data_set = []\n",
        "rand_list0 = random.sample(range(50), 50)\n",
        "rand_list1 = random.sample(range(50,100), 50)\n",
        "rand_list2 = random.sample(range(100,150), 50)\n",
        "\n",
        "for i in range(data_size//3):\n",
        "  data_set.append(lang_data['جملات'][rand_list0[i]]) #arab\n",
        "  data_set.append(lang_data['جملات'][rand_list1[i]]) #persain\n",
        "  data_set.append(lang_data['جملات'][rand_list2[i]]) #kor\n",
        "\n",
        "print(data_set[0])\n",
        "print(data_set[1])\n",
        "print(data_set[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TN-DDoAPNRf",
        "outputId": "e798157d-2b14-444c-ea87-0bade883808e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "الحكومة الإسرائيلية أقامت وحدات سكنية لاستيعاب المستوطنين\n",
            "اتسز نتوانست به توسعه قلمرو خوارزمشاهیان کمک چندانی بکند\n",
            "لە بیرمان بێ کە کاری زمانەوان ناساندنە ، نەوەک دانان\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lang_data_splitter(data, d_size, split, lang):\n",
        "  new_list = []\n",
        "  for i in range(d_size):\n",
        "    tmp = data[i].replace(' ','')\n",
        "    tmp = tmp.replace('\\xa0','')\n",
        "    tmp = tmp.replace('\\u200c','')\n",
        "    tmp = tmp.replace('،','')\n",
        "    if i%split == lang:\n",
        "      new_list.append(tmp)\n",
        "  return new_list"
      ],
      "metadata": {
        "id": "q7GtMzM7dabV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### bag of words"
      ],
      "metadata": {
        "id": "-7ZsKfaHAW5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bagAlphabet(arb_train_set, per_train_set, kor_train_set):\n",
        "  bag_alphabet = []\n",
        "  for i in arb_train_set:\n",
        "    for j in i:\n",
        "      if j not in bag_alphabet:\n",
        "        bag_alphabet.append(j)\n",
        "\n",
        "  for i in per_train_set:\n",
        "    for j in i:\n",
        "      if j not in bag_alphabet:\n",
        "        bag_alphabet.append(j)\n",
        "\n",
        "  for i in kor_train_set:\n",
        "    for j in i:\n",
        "      if j not in bag_alphabet:\n",
        "        bag_alphabet.append(j)\n",
        "  # print(bag_alphabet)\n",
        "  return bag_alphabet"
      ],
      "metadata": {
        "id": "y66wUiz4Anjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4 - Train"
      ],
      "metadata": {
        "id": "bpqS5Y5QpPqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Computing the normal CF"
      ],
      "metadata": {
        "id": "w7JwFwnoD824"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalCF(charFrequency_lang, bag_alphabet, train_size):\n",
        "  charFrequency, lang_normalCF = dict_temp(bag_alphabet)\n",
        "  for i in charFrequency.keys():\n",
        "    for j in range(train_size):\n",
        "      ncf = charFrequency_lang[i][j]/len(charFrequency_lang[i])\n",
        "      lang_normalCF[i].append(ncf)\n",
        "\n",
        "  lang_sumNFC = charFrequency.copy()\n",
        "  for i in charFrequency.keys():\n",
        "    tmp = 0\n",
        "    for j in range(train_size):\n",
        "      tmp += lang_normalCF[i][j]\n",
        "    lang_sumNFC[i] = tmp/train_size\n",
        "  # print(lang_sumNFC)\n",
        "  \n",
        "  return lang_sumNFC"
      ],
      "metadata": {
        "id": "gAcxK8zpD6gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 4 - Train with Character Frequency (CF)"
      ],
      "metadata": {
        "id": "i9hRQYf2AccI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_set, train_size):\n",
        "  arb_train_set = lang_data_splitter(train_set, train_size, 3, 0)\n",
        "  per_train_set = lang_data_splitter(train_set, train_size, 3, 1)\n",
        "  kor_train_set = lang_data_splitter(train_set, train_size, 3, 2)\n",
        "  train_size = len(arb_train_set)\n",
        "  # print(per_train_set)\n",
        "\n",
        "  bag_alphabet = bagAlphabet(arb_train_set, per_train_set, kor_train_set)\n",
        "\n",
        "  charFrequency_arb = charFC(arb_train_set, bag_alphabet)\n",
        "  charFrequency_per = charFC(per_train_set, bag_alphabet)\n",
        "  charFrequency_kor = charFC(kor_train_set, bag_alphabet)\n",
        "  # print(len(charFrequency_kor['ڤ']))\n",
        "  # print(len(charFrequency_per['ڤ']))\n",
        "  # print(len(charFrequency_arb['ڤ']))\n",
        "\n",
        "  arb_sumNFC = normalCF(charFrequency_arb, bag_alphabet, train_size)\n",
        "  per_sumNFC = normalCF(charFrequency_per, bag_alphabet, train_size)\n",
        "  kor_sumNFC = normalCF(charFrequency_kor, bag_alphabet, train_size)\n",
        "  \n",
        "  return arb_sumNFC, per_sumNFC, kor_sumNFC, bag_alphabet"
      ],
      "metadata": {
        "id": "FQdRYCJ7kOoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Character Frequency (CF)"
      ],
      "metadata": {
        "id": "_2Bfd4GFBW_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_temp(bag_alphabet):\n",
        "  charFrequency_lang = {}\n",
        "  charFrequency = {}\n",
        "  for i in bag_alphabet:\n",
        "    if i not in charFrequency_lang.keys():\n",
        "      charFrequency_lang[i] = [] #list of repeatation for evrey sentence\n",
        "      charFrequency[i] = int(0) \n",
        "  # print(len(charFrequency))\n",
        "  return charFrequency, charFrequency_lang"
      ],
      "metadata": {
        "id": "3CDiCwlWm6pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def charFC(t_data, bag_alphabet):\n",
        "  charFrequency, charFrequency_lang = dict_temp(bag_alphabet)\n",
        "  for i in t_data:\n",
        "    i = i.replace(' ','')\n",
        "    i = i.replace('\\xa0','')\n",
        "    i = i.replace('\\u200c','')\n",
        "    i = i.replace('،','')\n",
        "    # print(i)\n",
        "    for j in i:\n",
        "      if j in charFrequency.keys():\n",
        "        charFrequency[j] += int(1)\n",
        "    for k in charFrequency.keys():\n",
        "      charFrequency_lang[k].append(charFrequency[k])\n",
        "  return charFrequency_lang"
      ],
      "metadata": {
        "id": "vfx9E_q3CLiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5 - Test"
      ],
      "metadata": {
        "id": "_B7qJTViwVoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ncf_testing(charFrequency_lang, bag_alphabet, test_size):\n",
        "  charFrequency, lang_normalCF = dict_temp(bag_alphabet)\n",
        "  for i in charFrequency.keys():\n",
        "    for j in range(test_size):\n",
        "      ncf = charFrequency_lang[i][j]/len(charFrequency_lang[i])\n",
        "      lang_normalCF[i].append(ncf)\n",
        "  return lang_normalCF"
      ],
      "metadata": {
        "id": "l6fe6ZXSmEQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(ncf_test, ncf_arb, ncf_per, ncf_kor):\n",
        "  arb_Euclidean, per_Euclidean, kor_Euclidean = 0, 0, 0\n",
        "  for i in ncf_test.keys():\n",
        "    # print(ncf_arb[i],ncf_test[i])\n",
        "    tmp = pow(ncf_arb[i] - ncf_test[i], 2)\n",
        "    arb_Euclidean += tmp\n",
        "    tmp = pow(ncf_per[i] - ncf_test[i], 2)\n",
        "    per_Euclidean += tmp\n",
        "    tmp = pow(ncf_kor[i] - ncf_test[i], 2)\n",
        "    kor_Euclidean += tmp\n",
        "\n",
        "  arb_Euclidean = math.sqrt(arb_Euclidean)\n",
        "  per_Euclidean = math.sqrt(per_Euclidean)\n",
        "  kor_Euclidean = math.sqrt(kor_Euclidean)\n",
        "  # print(arb_Euclidean, per_Euclidean, kor_Euclidean)\n",
        "  tmp = min(arb_Euclidean, per_Euclidean, kor_Euclidean)\n",
        "  if tmp == arb_Euclidean:\n",
        "    result = 'Arabic'\n",
        "  elif tmp == per_Euclidean:\n",
        "    result = 'Persain'\n",
        "  elif tmp == kor_Euclidean:\n",
        "    result = 'Kordish'\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "3X210vUkqQFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resultFunc(test_data, bag_alphabet, arb_sumNFC, per_sumNFC, kor_sumNFC):\n",
        "  charFrequency, lang_normalCF = dict_temp(bag_alphabet)\n",
        "  charFrequency_tmp = charFC(test_data, bag_alphabet)\n",
        "  ncf_test_tmp = ncf_testing(charFrequency_tmp, bag_alphabet, len(test_data))\n",
        "  \n",
        "  test_result_list = []\n",
        "  for i in range(len(test_data)):\n",
        "    charFrequencytest = charFrequency.copy()\n",
        "    for j in ncf_test_tmp.keys():\n",
        "      charFrequencytest[j] = ncf_test_tmp[j][i]\n",
        "    tmp = predict(charFrequencytest, arb_sumNFC, per_sumNFC, kor_sumNFC)\n",
        "    test_result_list.append(tmp)\n",
        "  return test_result_list"
      ],
      "metadata": {
        "id": "d5hwwNCCJ057"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 5 - Test with with Character Frequency (CF) "
      ],
      "metadata": {
        "id": "1hZ2bKBlIMwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_set, test_size, bag_alphabet, arb_sumNFC, per_sumNFC, kor_sumNFC):\n",
        "  arb_test_set = lang_data_splitter(test_set, test_size, 3, 0)\n",
        "  per_test_set = lang_data_splitter(test_set, test_size, 3, 1)\n",
        "  kor_test_set = lang_data_splitter(test_set, test_size, 3, 2)\n",
        "\n",
        "  test_result_arab = resultFunc(arb_test_set, bag_alphabet, arb_sumNFC, per_sumNFC, kor_sumNFC)\n",
        "  test_result_per = resultFunc(per_test_set, bag_alphabet, arb_sumNFC, per_sumNFC, kor_sumNFC)\n",
        "  test_result_kor = resultFunc(kor_test_set, bag_alphabet, arb_sumNFC, per_sumNFC, kor_sumNFC)\n",
        "\n",
        "  return test_result_arab, test_result_per, test_result_kor"
      ],
      "metadata": {
        "id": "EI3IWJCxsmqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step6 - Accuracy / F1-measure / Recall / Precision"
      ],
      "metadata": {
        "id": "vRAQYUGZ5VJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Accuracy = TN + TP / Total"
      ],
      "metadata": {
        "id": "TLHr56FjBk6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(test_result_arab, test_result_per, test_result_kor):\n",
        "  total = 30\n",
        "  Tpos1, Tpos2, Tpos3 = 0, 0, 0\n",
        "  Tneg1, Tneg2, Tneg3 = 0, 0, 0\n",
        "\n",
        "  for i in test_result_arab:\n",
        "    if i == 'Arabic':\n",
        "      Tpos1 += 1\n",
        "      Tneg3 += 1\n",
        "      Tneg2 += 1\n",
        "    elif i == 'Persain':\n",
        "      Tneg3 += 1\n",
        "    else:\n",
        "      Tneg2 += 1\n",
        "\n",
        "  for i in test_result_per:\n",
        "    if i == 'Persain':\n",
        "      Tpos2 += 1\n",
        "      Tneg3 += 1\n",
        "      Tneg1 += 1\n",
        "    elif i == 'Arabic':\n",
        "      Tneg3 += 1\n",
        "    else:\n",
        "      Tneg1 += 1\n",
        "\n",
        "  for i in test_result_kor:\n",
        "    if i == 'Kordish':\n",
        "      Tpos3 += 1\n",
        "      Tneg1 += 1\n",
        "      Tneg2 += 1\n",
        "    elif i == 'Persain':\n",
        "      Tneg1 += 1\n",
        "    else:\n",
        "      Tneg2 += 1\n",
        "\n",
        "  accuracy_arab = (Tpos1+Tneg1)/total\n",
        "  accuracy_per = (Tpos2+Tneg2)/total\n",
        "  accuracy_kor = (Tpos3+Tneg3)/total\n",
        "\n",
        "  print('Accuracy:')\n",
        "  print('Accuracy for Arabic is', accuracy_arab)\n",
        "  print('Accuracy for Persain is', accuracy_per)\n",
        "  print('Accuracy for Kordish is', accuracy_kor)\n",
        "\n",
        "  return Tpos1, Tpos2, Tpos3, accuracy_arab, accuracy_per, accuracy_kor"
      ],
      "metadata": {
        "id": "G0R5iJur59AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Recall = TP/TP+FN\n"
      ],
      "metadata": {
        "id": "LntWuwrVFCQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recall(Tpos1, Tpos2, Tpos3):\n",
        "  recall_arab = Tpos1/10 #10 - FN = TP\n",
        "  recall_per = Tpos2/10\n",
        "  recall_kor = Tpos3/10\n",
        "  print('Recall:')\n",
        "  print('Recall for Arabic is', recall_arab)\n",
        "  print('Recall for Persain is', recall_per)\n",
        "  print('Recall for Kordish is', recall_kor)\n",
        "\n",
        "  return recall_arab, recall_per, recall_kor"
      ],
      "metadata": {
        "id": "7slljwb-FdRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Precision = TP / TP + FP"
      ],
      "metadata": {
        "id": "NNj6BNfwP_84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def percision(test_result_arab, test_result_per, test_result_kor):\n",
        "  total = 30\n",
        "  Tpos1, Tpos2, Tpos3 = 0, 0, 0\n",
        "  fpos1, fpos2, fpos3 = 0, 0, 0\n",
        "\n",
        "  for i in test_result_arab:\n",
        "    if i == 'Arabic':\n",
        "      Tpos1 += 1\n",
        "    elif i == 'Persain':\n",
        "      fpos2 += 1\n",
        "    else:\n",
        "      fpos3 += 1\n",
        "\n",
        "  for i in test_result_per:\n",
        "    if i == 'Persain':\n",
        "      Tpos2 += 1\n",
        "    elif i == 'Arabic':\n",
        "      fpos1 += 1\n",
        "    else:\n",
        "      fpos3 += 1\n",
        "\n",
        "  for i in test_result_kor:\n",
        "    if i == 'Kordish':\n",
        "      Tpos3 += 1\n",
        "    elif i == 'Persain':\n",
        "      fpos2 += 1\n",
        "    else:\n",
        "      fpos1 += 1\n",
        "\n",
        "  # print('Tpos1+fpos1', Tpos1+fpos1)\n",
        "  precision_arab = Tpos1/(Tpos1+fpos1)\n",
        "  precision_per = Tpos2/(Tpos2+fpos2)\n",
        "  precision_kor = Tpos3/(Tpos3+fpos3)\n",
        "\n",
        "  print('Precision:')\n",
        "  print('Precision for Arabic is', precision_arab)\n",
        "  print('Precision for Persain is', precision_per)\n",
        "  print('Precision for Kordish is', precision_kor)\n",
        "\n",
        "  return precision_arab, precision_per, precision_kor"
      ],
      "metadata": {
        "id": "0Pk9oVIQ8Mkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####F1-Measure = 2 * precision * recall / ( precision + recall )"
      ],
      "metadata": {
        "id": "YgmWnvPaRn-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fMeasure(lang, precision_lang, recall_lang):\n",
        "  FMeasure_lang = 2 * precision_lang * recall_lang / (precision_lang + recall_lang)\n",
        "  print('F1-Measure for', lang, 'is', FMeasure_lang)\n",
        "  return FMeasure_lang"
      ],
      "metadata": {
        "id": "DpwvNRZ285zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 7 - Evaluate  "
      ],
      "metadata": {
        "id": "dqcIaNU92jtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(test_result_arab, test_result_per, test_result_kor):\n",
        "  print('___________________________________________________________________')\n",
        "  \n",
        "  eval_dic = {}\n",
        "\n",
        "  Tpos1, Tpos2, Tpos3, accuracy_arab, accuracy_per, accuracy_kor = accuracy(test_result_arab, test_result_per, test_result_kor)\n",
        "\n",
        "  try:\n",
        "    recall_arab, recall_per, recall_kor = recall(Tpos1, Tpos2, Tpos3)\n",
        "  except:\n",
        "    print('RECALL:float division by zero')\n",
        "\n",
        "  try:\n",
        "    precision_arab, precision_per, precision_kor = percision(test_result_arab, test_result_per, test_result_kor)\n",
        "  except:\n",
        "    print('PRECISION:float division by zero')\n",
        "\n",
        "  print('F1-Measure:')\n",
        "  try:\n",
        "    FMeasure_arab = fMeasure('Arabic', precision_arab, recall_arab)\n",
        "  except:\n",
        "    print('Arabic:float division by zero')\n",
        "  try:\n",
        "    FMeasure_per = fMeasure('Persain', precision_per, recall_per)\n",
        "  except:\n",
        "    print('Persain:float division by zero')\n",
        "  try:\n",
        "    FMeasure_kor = fMeasure('Kordish', precision_kor, recall_kor)\n",
        "  except:\n",
        "    print('Kordish:float division by zero')\n",
        "\n",
        "  eval_dic['arb'] = [accuracy_arab, recall_arab, precision_arab, FMeasure_arab]\n",
        "  eval_dic['per'] = [accuracy_per, recall_per, precision_per, FMeasure_per]\n",
        "  eval_dic['kor'] = [accuracy_kor, recall_kor, precision_kor, FMeasure_kor]\n",
        "\n",
        "  return eval_dic"
      ],
      "metadata": {
        "id": "yOq30tmFrSSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RUN THE WHOLE CODE"
      ],
      "metadata": {
        "id": "mS_ksXqGBYMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc1, pre1, rec1, fmsr1 = 0, 0, 0, 0 #arabic\n",
        "acc2, pre2, rec2, fmsr2 = 0, 0, 0, 0 #persain\n",
        "acc3, pre3, rec3, fmsr3 = 0, 0, 0, 0 #kordish\n",
        "\n",
        "for i in range(k_fold):\n",
        "  test_set = []\n",
        "  train_set = []\n",
        "  test_set = data_set[i*test_size:i*test_size+30]\n",
        "  train_set = data_set[:i*test_size]\n",
        "  train_set += data_set[i*test_size+30:]\n",
        "  # print(len(train_set))\n",
        "\n",
        "  arb_sumNFC, per_sumNFC, kor_sumNFC, bag_alphabet = train(train_set, train_size)\n",
        "\n",
        "\n",
        "  test_result_arab, test_result_per, test_result_kor = test(test_set, test_size, bag_alphabet, arb_sumNFC, per_sumNFC, kor_sumNFC)\n",
        "  print(f'________________________FOLD {i}_________________________________')\n",
        "  evaluate_dic = evaluate(test_result_arab, test_result_per, test_result_kor)\n",
        "\n",
        "  acc1 += evaluate_dic['arb'][0]\n",
        "  acc2 += evaluate_dic['per'][0]\n",
        "  acc3 += evaluate_dic['kor'][0]\n",
        "\n",
        "  rec1 += evaluate_dic['arb'][1]\n",
        "  rec2 += evaluate_dic['per'][1]\n",
        "  rec3 += evaluate_dic['kor'][1]\n",
        "\n",
        "  pre1 += evaluate_dic['arb'][2]\n",
        "  pre2 += evaluate_dic['per'][2]\n",
        "  pre3 += evaluate_dic['kor'][2]\n",
        "\n",
        "  fmsr1 += evaluate_dic['arb'][3]\n",
        "  fmsr2 += evaluate_dic['per'][3]\n",
        "  fmsr3 += evaluate_dic['kor'][3]\n",
        "\n",
        "print('_____________________________________________________________________')\n",
        "print('______________________Version3_CF & K fold___________________________')\n",
        "print('______________________________ALL____________________________________')\n",
        "\n",
        "print('Accuracy:')\n",
        "print('Accuracy for Arabic is', acc1/k_fold)\n",
        "print('Accuracy for Persain is', acc2/k_fold)\n",
        "print('Accuracy for Kordish is', acc3/k_fold)\n",
        "\n",
        "print('Recall:')\n",
        "print('Recall for Arabic is', rec1/k_fold)\n",
        "print('Recall for Persain is', rec2/k_fold)\n",
        "print('Recall for Kordish is', rec3/k_fold)\n",
        "\n",
        "print('Precision:')\n",
        "print('Precision for Arabic is', pre1/k_fold)\n",
        "print('Precision for Persain is', pre2/k_fold)\n",
        "print('Precision for Kordish is', pre3/k_fold)\n",
        "\n",
        "print('F1-Measure:')\n",
        "print('F1-Measure for Arabic is', fmsr1/k_fold)\n",
        "print('F1-Measure for Persain is', fmsr2/k_fold)\n",
        "print('F1-Measure for Kordish is', fmsr3/k_fold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01pla0B_WkGj",
        "outputId": "17921630-2392-4af3-a699-791e044f1147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "________________________FOLD 0_________________________________\n",
            "_____________________________________________________________________\n",
            "Accuracy:\n",
            "Accuracy for Arabic is 1.0\n",
            "Accuracy for Persain is 0.9333333333333333\n",
            "Accuracy for Kordish is 0.9333333333333333\n",
            "Recall:\n",
            "Recall for Arabic is 1.0\n",
            "Recall for Persain is 1.0\n",
            "Recall for Kordish is 0.8\n",
            "Precision:\n",
            "Precision for Arabic is 1.0\n",
            "Precision for Persain is 0.8333333333333334\n",
            "Precision for Kordish is 1.0\n",
            "F1-Measure:\n",
            "F1-Measure for Arabic is 1.0\n",
            "F1-Measure for Persain is 0.9090909090909091\n",
            "F1-Measure for Kordish is 0.888888888888889\n",
            "________________________FOLD 1_________________________________\n",
            "_____________________________________________________________________\n",
            "Accuracy:\n",
            "Accuracy for Arabic is 1.0\n",
            "Accuracy for Persain is 0.9\n",
            "Accuracy for Kordish is 0.9\n",
            "Recall:\n",
            "Recall for Arabic is 1.0\n",
            "Recall for Persain is 1.0\n",
            "Recall for Kordish is 0.7\n",
            "Precision:\n",
            "Precision for Arabic is 1.0\n",
            "Precision for Persain is 0.7692307692307693\n",
            "Precision for Kordish is 1.0\n",
            "F1-Measure:\n",
            "F1-Measure for Arabic is 1.0\n",
            "F1-Measure for Persain is 0.8695652173913044\n",
            "F1-Measure for Kordish is 0.8235294117647058\n",
            "________________________FOLD 2_________________________________\n",
            "_____________________________________________________________________\n",
            "Accuracy:\n",
            "Accuracy for Arabic is 1.0\n",
            "Accuracy for Persain is 1.0\n",
            "Accuracy for Kordish is 1.0\n",
            "Recall:\n",
            "Recall for Arabic is 1.0\n",
            "Recall for Persain is 1.0\n",
            "Recall for Kordish is 1.0\n",
            "Precision:\n",
            "Precision for Arabic is 1.0\n",
            "Precision for Persain is 1.0\n",
            "Precision for Kordish is 1.0\n",
            "F1-Measure:\n",
            "F1-Measure for Arabic is 1.0\n",
            "F1-Measure for Persain is 1.0\n",
            "F1-Measure for Kordish is 1.0\n",
            "________________________FOLD 3_________________________________\n",
            "_____________________________________________________________________\n",
            "Accuracy:\n",
            "Accuracy for Arabic is 0.9666666666666667\n",
            "Accuracy for Persain is 0.9666666666666667\n",
            "Accuracy for Kordish is 1.0\n",
            "Recall:\n",
            "Recall for Arabic is 1.0\n",
            "Recall for Persain is 0.9\n",
            "Recall for Kordish is 1.0\n",
            "Precision:\n",
            "Precision for Arabic is 0.9090909090909091\n",
            "Precision for Persain is 1.0\n",
            "Precision for Kordish is 1.0\n",
            "F1-Measure:\n",
            "F1-Measure for Arabic is 0.9523809523809523\n",
            "F1-Measure for Persain is 0.9473684210526316\n",
            "F1-Measure for Kordish is 1.0\n",
            "________________________FOLD 4_________________________________\n",
            "_____________________________________________________________________\n",
            "Accuracy:\n",
            "Accuracy for Arabic is 1.0\n",
            "Accuracy for Persain is 0.9666666666666667\n",
            "Accuracy for Kordish is 0.9666666666666667\n",
            "Recall:\n",
            "Recall for Arabic is 1.0\n",
            "Recall for Persain is 1.0\n",
            "Recall for Kordish is 0.9\n",
            "Precision:\n",
            "Precision for Arabic is 1.0\n",
            "Precision for Persain is 0.9090909090909091\n",
            "Precision for Kordish is 1.0\n",
            "F1-Measure:\n",
            "F1-Measure for Arabic is 1.0\n",
            "F1-Measure for Persain is 0.9523809523809523\n",
            "F1-Measure for Kordish is 0.9473684210526316\n",
            "_____________________________________________________________________\n",
            "______________________Version3_CF & K fold___________________________\n",
            "______________________________ALL____________________________________\n",
            "Accuracy:\n",
            "Accuracy for Arabic is 0.9933333333333334\n",
            "Accuracy for Persain is 0.9533333333333334\n",
            "Accuracy for Kordish is 0.96\n",
            "Recall:\n",
            "Recall for Arabic is 1.0\n",
            "Recall for Persain is 0.9800000000000001\n",
            "Recall for Kordish is 0.8800000000000001\n",
            "Precision:\n",
            "Precision for Arabic is 0.9818181818181818\n",
            "Precision for Persain is 0.9023310023310023\n",
            "Precision for Kordish is 1.0\n",
            "F1-Measure:\n",
            "F1-Measure for Arabic is 0.9904761904761905\n",
            "F1-Measure for Persain is 0.9356810999831595\n",
            "F1-Measure for Kordish is 0.9319573443412453\n"
          ]
        }
      ]
    }
  ]
}